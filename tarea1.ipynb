{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to move data to GPU\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(d, device) for d in data]\n",
    "    else:\n",
    "        return data.to(device, non_blocking=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura Xception"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 1\n",
    "Investigue y explique la mejora que introduce la arquitectura Xception en relación a su predecesor Inception.\n",
    "Como hint, estudie el concepto de Depthwise Separable Convolution y cómo impacta esto en el tiempo de entrenamiento de la red."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xception introduce dos mejoras principales:\n",
    "\n",
    "**Depthwise Separable Convolution**: es una alternativa a la convolución clásica para ser más eficiente en términos de tiempo de computación. Se divide en dos pasos:\n",
    "* Depthwise convolution: es un primer paso donde en vez de aplicar una convolución $d \\times d \\times C$ donde $d$ es el tamaño del kernel y $C$ el número de canales, aplica una convolución $d \\times d \\times 1$ es decir a un solo canal. \n",
    "* Pointwise convolution: opera una convolución clásica de tamaño $1 \\times 1 \\times N$ donde $N$ es el número de kernels.\n",
    "Siguiendo este procedimiento se reduce el número de operaciones en un factor propocional a $1/N$.\n",
    "\n",
    "**Shortcuts entre bloques de convolución como en ResNet**: implementa el uso de bloques residuales, es decir que cada capa alimenta a la siguiente capa y también de manera directa saltando algunas capas intermedias. La idea detrás es evitar el \"vanishing gradient\" y poder entrenar redes más profundas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 2\n",
    "Explique, en no más de tres lı́neas cada uno, la utilidad o función que cumplen en la arquitectura de una CNNs:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Capa densa (Fully connected layer)**\n",
    "Las capas densas usualmente son ubicadas a la salida de la red con fines de clasificación. Usualmente tiene el mismo número de nodos de salida que el número de clases y junto a una función softmax lo cual nos permite interpretar los valores de esta última capa como la probabilidad de pertenecer a una cierta clase.\n",
    "\n",
    "**2. Kernel de convolución 1x1**\n",
    "El uso principal es el cambio de dimensionalidad en el espacio del filtro. Si el número de filtros convolucionales de salida es mayor que el de entrada $F_1 > F$ se incrementa la dimensionalidad, caso contrario $F_1 < F$, se reduce. Reducir la dimensionalidad reduce el costo computacional. \n",
    "\n",
    "**3. MaxPooling**\n",
    "MaxPooling es usado para reducir la \"resolución\" de una capa convolucional, la red estará \"mirando\" áreas más grandes de la imagen y reducirá el número de parámetros, por lo tanto reducirá el costo computacional. También ayudará a \"ver\" los pixeles más activados o importantes descartando los otros.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 3\n",
    "Como primera actividad, tendrán que recrear la operación de Depthwise Separable Convolution rellenando el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, padding=1, bias=False, **kwargs\n",
    "    ):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "\n",
    "        # aplicamos una convolución por canal\n",
    "        self.depthwise_conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,\n",
    "            groups=in_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.pointwise_conv = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # No cambiar\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, responda explı́citamente: ¿cuáles son los valores de los parámetros kernel_size y stride en\n",
    "la operación self.pointwise_conv?.\n",
    "\n",
    "* kernel_size = 1 (filtros de 1 x 1)\n",
    "* stride = 1 (por defecto)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 4\n",
    "Compare el número de parámetros entre un bloque de SeparableConv2d y una convolución para un vol-\n",
    "umen de entrada de forma [3, 300, 300]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando un out_channel = 10, tenemos:\n",
    "* SeparableConv2d: 57 parámetros\n",
    "* Classic Conv2d: 270 parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 300, 300]              27\n",
      "            Conv2d-2         [-1, 10, 300, 300]              30\n",
      "   SeparableConv2d-3         [-1, 10, 300, 300]               0\n",
      "================================================================\n",
      "Total params: 57\n",
      "Trainable params: 57\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.03\n",
      "Forward/backward pass size (MB): 15.79\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 16.82\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_sep = nn.Sequential(SeparableConv2d(3, 10, 3))\n",
    "summary(model_sep.cuda(), input_size=(3, 300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 10, 298, 298]             270\n",
      "================================================================\n",
      "Total params: 270\n",
      "Trainable params: 270\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.03\n",
      "Forward/backward pass size (MB): 6.78\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 7.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_classic = nn.Sequential(\n",
    "    nn.Conv2d(3, 10, kernel_size=3, padding=\"valid\", bias=False)\n",
    ")\n",
    "summary(model_classic.cuda(), input_size=(3, 300, 300))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 5\n",
    "En esta segunda actividad crearemos el bloque base de la red, que se compone de etapas subsecuentes de la operación SeparableConv2d además de ReLU, BatchNorm y MaxPooling, junto con una conexión\n",
    "skip-forward que varı́a según la etapa donde nos encontramos. Para esto, tendrá que completar la siguiente clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        start_with_relu=True,\n",
    "        residual_connection=False,\n",
    "        residual_type=\"Conv2d\",\n",
    "        num_subblocks=2,\n",
    "        last_layer=\"MaxPooling\",\n",
    "        padding_conv2d=[1, 1],\n",
    "        padding_residual=1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(XceptionBlock, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "        self.residual_connection = residual_connection\n",
    "        self.residual_type = residual_type\n",
    "        # # Agregue los módulos según corresponda\n",
    "        # # ...\n",
    "\n",
    "        # self.modules = nn.Sequential(*modules)\n",
    "\n",
    "        # Agregamos sub-bloques segun corresponda\n",
    "        #\n",
    "        for i in range(num_subblocks):\n",
    "            modules.append(nn.ReLU(inplace=True))\n",
    "            if i == 0:\n",
    "                modules.append(\n",
    "                    SeparableConv2d(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=padding_conv2d,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                modules.append(\n",
    "                    SeparableConv2d(\n",
    "                        out_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=padding_conv2d,\n",
    "                    )\n",
    "                )\n",
    "            modules.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        #     ReLU -> SeparableConv2d -> BatchNormalization\n",
    "        # Admite no incluir la ReLU mediante el parametro \"start_with_relu\"\n",
    "        #\n",
    "\n",
    "        # Agregamos layer final segun corresponda\n",
    "        if last_layer == \"MaxPooling\":\n",
    "            modules.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
    "\n",
    "        # Agregamos conexion skip-forward\n",
    "        if self.residual_type == \"Conv2d\":\n",
    "            self.mod_residual = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=2,\n",
    "                    padding=padding_residual,\n",
    "                ),\n",
    "                # nn.BatchNorm2d(out_channels).to(get_default_device())\n",
    "            )\n",
    "        if self.residual_type == \"Pass\":\n",
    "            self.mod_residual = None\n",
    "\n",
    "        # Definimos si partimos o no con ReLU\n",
    "        if not start_with_relu:\n",
    "            modules = modules[1:]\n",
    "\n",
    "        self.mods = nn.Sequential(*modules).to(get_default_device())\n",
    "\n",
    "    # def _subblock(self, in_channels, out_channels):\n",
    "    #     \"\"\"\n",
    "    #     Construye un sub-bloque compuesto de:\n",
    "    #         ReLU -> SeparableConv2d -> BatchNormalization\n",
    "\n",
    "    #     Admite no incluir la ReLU mediante el parametro \"start_with_relu\"\n",
    "    #     \"\"\"\n",
    "    #     modules = [\n",
    "    #         nn.ReLU(inplace=True),\n",
    "    #         SeparableConv2d(\n",
    "    #             in_channels, out_channels, kernel_size=3, stride=1),\n",
    "    #         nn.BatchNorm2d(out_channels)\n",
    "    #     ]\n",
    "\n",
    "    #     return modules\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass de un bloque.\n",
    "        \"\"\"\n",
    "        # print(x)\n",
    "        # No cambiar\n",
    "        Fx = self.mods(x)\n",
    "\n",
    "        if self.residual_connection:\n",
    "            if self.residual_type == \"Conv2d\":\n",
    "                residual_calc = self.mod_residual(x)\n",
    "                Fx += residual_calc\n",
    "\n",
    "            if self.residual_type == \"Pass\":\n",
    "                Fx += x\n",
    "\n",
    "        return Fx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 6\n",
    "Por último, complete la clase Xception que instancia el modelo completo. Para esto, tendrá que usar la clase XceptionBlock, instanciando cada etapa por separado en la inicialización de la clase y rellenando las funciones para cada flujo. Note que el método Xception.forward ya está implementado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xception(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Xception, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        modules = []\n",
    "        self.modules = nn.Sequential(*modules)\n",
    "        # Head de clasificacion para ilustrar\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        # Aqui debe agregar todos los bloques y etapas\n",
    "        # que necesitara para las funciones de cada flujo.\n",
    "        # ...\n",
    "\n",
    "    def entry_flow(self, x):\n",
    "        \"\"\"\n",
    "        Flujo de entrada (debera usar la funcion\n",
    "        XceptionBlock tres veces mas las dos capas\n",
    "        convolucionales vistas en el enunciado)\n",
    "        \"\"\"\n",
    "        # Agregar flujo de entrada\n",
    "        # ...\n",
    "        # Primeros dos bloques\n",
    "        first_block = [\n",
    "            nn.Conv2d(\n",
    "                self.in_channels, 32, kernel_size=3, stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        second_block = [\n",
    "            XceptionBlock(\n",
    "                64,\n",
    "                128,\n",
    "                start_with_relu=False,\n",
    "                residual_connection=True,\n",
    "                residual_type=\"Conv2d\",\n",
    "                last_layer=\"MaxPooling\",\n",
    "                num_subblocks=2,\n",
    "                padding_conv2d=2,\n",
    "                padding_residual=1,\n",
    "            )\n",
    "        ]\n",
    "        third_block = [\n",
    "            XceptionBlock(\n",
    "                128,\n",
    "                256,\n",
    "                start_with_relu=True,\n",
    "                residual_connection=True,\n",
    "                residual_type=\"Conv2d\",\n",
    "                last_layer=\"MaxPooling\",\n",
    "                num_subblocks=2,\n",
    "                padding_conv2d=2,\n",
    "            )\n",
    "        ]\n",
    "        fourth_block = [\n",
    "            XceptionBlock(\n",
    "                256,\n",
    "                728,\n",
    "                start_with_relu=True,\n",
    "                residual_connection=True,\n",
    "                residual_type=\"Conv2d\",\n",
    "                last_layer=\"MaxPooling\",\n",
    "                num_subblocks=2,\n",
    "                padding_conv2d=2,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Agregue los módulos según corresponda\n",
    "        # ...\n",
    "        x = nn.Sequential(*first_block, *second_block, *third_block, *fourth_block).to(\n",
    "            get_default_device()\n",
    "        )(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def middle_flow(self, x):\n",
    "        \"\"\"\n",
    "        Flujo intermedio (recordar que se repite 8 veces\n",
    "        un XceptionBlock de determinados parametros)\n",
    "        \"\"\"\n",
    "        # Agregar flujo intermedio\n",
    "        # ...\n",
    "\n",
    "        x = nn.Sequential(\n",
    "            XceptionBlock(\n",
    "                728,\n",
    "                728,\n",
    "                start_with_relu=True,\n",
    "                residual_connection=True,\n",
    "                residual_type=\"Pass\",\n",
    "                last_layer=None,\n",
    "                num_subblocks=3,\n",
    "                padding_residual=0,\n",
    "            ),\n",
    "            XceptionBlock(\n",
    "                728,\n",
    "                728,\n",
    "                start_with_relu=True,\n",
    "                residual_connection=True,\n",
    "                residual_type=\"Pass\",\n",
    "                last_layer=None,\n",
    "                num_subblocks=3,\n",
    "            ),\n",
    "            XceptionBlock(\n",
    "                728,\n",
    "                728,\n",
    "                start_with_relu=True,\n",
    "                residual_connection=True,\n",
    "                residual_type=\"Pass\",\n",
    "                last_layer=None,\n",
    "                num_subblocks=3,\n",
    "            ),\n",
    "            XceptionBlock(\n",
    "                728,\n",
    "                728,\n",
    "                start_with_relu=True,\n",
    "                residual_connection=True,\n",
    "                residual_type=\"Pass\",\n",
    "                last_layer=None,\n",
    "                num_subblocks=3,\n",
    "            ),\n",
    "            XceptionBlock(\n",
    "                728,\n",
    "                728,\n",
    "                start_with_relu=True,\n",
    "                residual_connection=True,\n",
    "                residual_type=\"Pass\",\n",
    "                last_layer=None,\n",
    "                num_subblocks=3,\n",
    "            ),\n",
    "            XceptionBlock(\n",
    "                728,\n",
    "                728,\n",
    "                start_with_relu=True,\n",
    "                residual_connection=True,\n",
    "                residual_type=\"Pass\",\n",
    "                last_layer=None,\n",
    "                num_subblocks=3,\n",
    "            ),\n",
    "            XceptionBlock(\n",
    "                728,\n",
    "                728,\n",
    "                start_with_relu=True,\n",
    "                residual_connection=True,\n",
    "                residual_type=\"Pass\",\n",
    "                last_layer=None,\n",
    "                num_subblocks=3,\n",
    "            ),\n",
    "            XceptionBlock(\n",
    "                728,\n",
    "                728,\n",
    "                start_with_relu=True,\n",
    "                residual_connection=True,\n",
    "                residual_type=\"Pass\",\n",
    "                last_layer=None,\n",
    "                num_subblocks=3,\n",
    "            ),\n",
    "        ).to(get_default_device())(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def exit_flow(self, x):\n",
    "        \"\"\"\n",
    "        Flujo de salida (tener en cuenta cambio de\n",
    "        dimensiones entre sub-bloques y funcion de\n",
    "        salida)\n",
    "        \"\"\"\n",
    "        # Agregar flujo de salida\n",
    "        # ...\n",
    "        first_block = [\n",
    "            XceptionBlock(\n",
    "                728,\n",
    "                1024,\n",
    "                start_with_relu=True,\n",
    "                residual_connection=True,\n",
    "                last_layer=\"MaxPooling\",\n",
    "                num_subblocks=2,\n",
    "                padding_conv2d=2,\n",
    "            )\n",
    "        ]\n",
    "        second_block = [\n",
    "            XceptionBlock(\n",
    "                1024,\n",
    "                1536,\n",
    "                start_with_relu=False,\n",
    "                residual_connection=False,\n",
    "                last_layer=None,\n",
    "                num_subblocks=1,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            XceptionBlock(\n",
    "                1536,\n",
    "                2048,\n",
    "                start_with_relu=False,\n",
    "                residual_connection=False,\n",
    "                last_layer=None,\n",
    "                num_subblocks=1,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        # No olvidar etapa de Global Avg. Pooling\n",
    "        # Hint: https://discuss.pytorch.org/t/global-average-pooling-in-pytorch/6721/8\n",
    "        # x = GlobalAvgPooling(x)\n",
    "\n",
    "        # global_avg_pooling = [\n",
    "        #     nn.Dropout(p=0.5),\n",
    "        #     nn.Conv2d(2048, self.num_classes, kernel_size=1),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.AvgPool2d(3 * 2048 * self.num_classes),\n",
    "        # ]\n",
    "\n",
    "        x = nn.Sequential(*first_block, *second_block).to(get_default_device())(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.entry_flow(x)\n",
    "\n",
    "        x = self.middle_flow(x)\n",
    "        x = self.exit_flow(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "\n",
    "        # Hay que agregar un cambio de dimensiones para poder\n",
    "        # pasar x por la capa Fully Connected\n",
    "        # x = ...\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 7\n",
    "Usando la función torchsummary.summary(), visualice y verifique que los tamaños de los outputs de\n",
    "cada flujo sean los correctos. Explicite el nombre exacto de cada capa y sus dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xception = Xception(3, 2048)\n",
    "# summary(model_xception.to(get_default_device()), input_size=(3, 300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
      "=====================================================================================================================================================================\n",
      "Xception                                 [1, 3, 300, 300]          [1, 2048]                 --                        --                        --\n",
      "├─Linear: 1-1                            [1, 2048]                 [1, 2048]                 4,196,352                 --                        4,196,352\n",
      "│    └─weight                                                                                ├─4,194,304               [2048, 2048]\n",
      "│    └─bias                                                                                  └─2,048                   [2048]\n",
      "=====================================================================================================================================================================\n",
      "Total params: 4,196,352\n",
      "Trainable params: 4,196,352\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 4.20\n",
      "=====================================================================================================================================================================\n",
      "Input size (MB): 1.08\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 16.79\n",
      "Estimated Total Size (MB): 17.88\n",
      "=====================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "=====================================================================================================================================================================\n",
       "Xception                                 [1, 3, 300, 300]          [1, 2048]                 --                        --                        --\n",
       "├─Linear: 1-1                            [1, 2048]                 [1, 2048]                 4,196,352                 --                        4,196,352\n",
       "│    └─weight                                                                                ├─4,194,304               [2048, 2048]\n",
       "│    └─bias                                                                                  └─2,048                   [2048]\n",
       "=====================================================================================================================================================================\n",
       "Total params: 4,196,352\n",
       "Trainable params: 4,196,352\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 4.20\n",
       "=====================================================================================================================================================================\n",
       "Input size (MB): 1.08\n",
       "Forward/backward pass size (MB): 0.02\n",
       "Params size (MB): 16.79\n",
       "Estimated Total Size (MB): 17.88\n",
       "====================================================================================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(\n",
    "    model_xception.to(get_default_device()),\n",
    "    (3, 300, 300),\n",
    "    batch_dim=0,\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"),\n",
    "    verbose=2,\n",
    "    depth=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xception(\n",
       "  (modules): Sequential()\n",
       "  (fc): Linear(in_features=2048, out_features=2048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xception.to(get_default_device())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación y entrenamiento de un clasificador de perros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
