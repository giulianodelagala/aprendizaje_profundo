{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to move data to GPU\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(d, device) for d in data]\n",
    "    else:\n",
    "        return data.to(device, non_blocking=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura Xception"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 1\n",
    "Investigue y explique la mejora que introduce la arquitectura Xception en relación a su predecesor Inception.\n",
    "Como hint, estudie el concepto de Depthwise Separable Convolution y cómo impacta esto en el tiempo de entrenamiento de la red."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xception introduce dos mejoras principales:\n",
    "\n",
    "**Depthwise Separable Convolution**: es una alternativa a la convolución clásica para ser más eficiente en términos de tiempo de computación. Se divide en dos pasos:\n",
    "* Depthwise convolution: es un primer paso donde en vez de aplicar una convolución $d \\times d \\times C$ donde $d$ es el tamaño del kernel y $C$ el número de canales, aplica una convolución $d \\times d \\times 1$ es decir a un solo canal. \n",
    "* Pointwise convolution: opera una convolución clásica de tamaño $1 \\times 1 \\times N$ donde $N$ es el número de kernels.\n",
    "Siguiendo este procedimiento se reduce el número de operaciones en un factor propocional a $1/N$.\n",
    "\n",
    "**Shortcuts entre bloques de convolución como en ResNet**: implementa el uso de bloques residuales, es decir que cada capa alimenta a la siguiente capa y también de manera directa saltando algunas capas intermedias. La idea detrás es evitar el \"vanishing gradient\" y poder entrenar redes más profundas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 2\n",
    "Explique, en no más de tres lı́neas cada uno, la utilidad o función que cumplen en la arquitectura de una CNNs:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Capa densa (Fully connected layer)**\n",
    "Las capas densas usualmente son ubicadas a la salida de la red con fines de clasificación. Usualmente tiene el mismo número de nodos de salida que el número de clases y junto a una función softmax lo cual nos permite interpretar los valores de esta última capa como la probabilidad de pertenecer a una cierta clase.\n",
    "\n",
    "**2. Kernel de convolución 1x1**\n",
    "El uso principal es el cambio de dimensionalidad en el espacio del filtro. Si el número de filtros convolucionales de salida es mayor que el de entrada $F_1 > F$ se incrementa la dimensionalidad, caso contrario $F_1 < F$, se reduce. Reducir la dimensionalidad reduce el costo computacional. \n",
    "\n",
    "**3. MaxPooling**\n",
    "MaxPooling es usado para reducir la \"resolución\" de una capa convolucional, la red estará \"mirando\" áreas más grandes de la imagen y reducirá el número de parámetros, por lo tanto reducirá el costo computacional. También ayudará a \"ver\" los pixeles más activados o importantes descartando los otros.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 3\n",
    "Como primera actividad, tendrán que recrear la operación de Depthwise Separable Convolution rellenando el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=False, **kwargs):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        \n",
    "        # aplicamos una convolución por canal \n",
    "        self.depthwise_conv = nn.Conv2d(\n",
    "            in_channels, in_channels, groups=in_channels, kernel_size=kernel_size, padding='valid', bias=bias)\n",
    "        self.pointwise_conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1, padding='valid', bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # No cambiar\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, responda explı́citamente: ¿cuáles son los valores de los parámetros kernel_size y stride en\n",
    "la operación self.pointwise_conv?.\n",
    "\n",
    "* kernel_size = 1 (filtros de 1 x 1)\n",
    "* stride = 1 (por defecto)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 4\n",
    "Compare el número de parámetros entre un bloque de SeparableConv2d y una convolución para un vol-\n",
    "umen de entrada de forma [3, 300, 300]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando un out_channel = 10, tenemos:\n",
    "* SeparableConv2d: 57 parámetros\n",
    "* Classic Conv2d: 270 parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 298, 298]              27\n",
      "            Conv2d-2         [-1, 10, 298, 298]              30\n",
      "   SeparableConv2d-3         [-1, 10, 298, 298]               0\n",
      "================================================================\n",
      "Total params: 57\n",
      "Trainable params: 57\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.03\n",
      "Forward/backward pass size (MB): 15.58\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 16.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_sep = nn.Sequential(SeparableConv2d(3,10,3))\n",
    "summary(model_sep.cuda(), input_size=(3,300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 10, 298, 298]             270\n",
      "================================================================\n",
      "Total params: 270\n",
      "Trainable params: 270\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.03\n",
      "Forward/backward pass size (MB): 6.78\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 7.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_classic = nn.Sequential(\n",
    "    nn.Conv2d(3, 10, kernel_size=3, padding='valid', bias=False))\n",
    "summary(model_classic.cuda(), input_size=(3,300,300))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 5\n",
    "En esta segunda actividad crearemos el bloque base de la red, que se compone de etapas subsecuentes de la operación SeparableConv2d además de ReLU, BatchNorm y MaxPooling, junto con una conexión\n",
    "skip-forward que varı́a según la etapa donde nos encontramos. Para esto, tendrá que completar la siguiente clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, start_with_relu=True,\n",
    "                 residual_connection=False, num_subblocks=2,\n",
    "                 last_layer='MaxPooling', **kwargs):\n",
    "        super(XceptionBlock, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "        # # Agregue los módulos según corresponda\n",
    "        # # ...\n",
    "\n",
    "        # self.modules = nn.Sequential(*modules)\n",
    "\n",
    "        # Agregamos sub-bloques segun corresponda\n",
    "        for _ in range(num_subblocks):\n",
    "            modules += self._subblock(out_channels, out_channels)\n",
    "\n",
    "        # Agregamos layer final segun corresponda\n",
    "        if last_layer == 'MaxPooling':\n",
    "            modules.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "        # Agregamos conexion skip-forward\n",
    "        if residual_connection:\n",
    "            self.skip_forward = None\n",
    "        else:\n",
    "            self.skip_forward = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        # Definimos si partimos o no con ReLU\n",
    "        if not start_with_relu:\n",
    "            modules = modules[1:]\n",
    "\n",
    "        self.modules = nn.Sequential(*modules)\n",
    "\n",
    "    def _subblock(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Construye un sub-bloque compuesto de:\n",
    "            ReLU -> SeparableConv2d -> BatchNormalization \n",
    "\n",
    "        Admite no incluir la ReLU mediante el parametro \"start_with_relu\"\n",
    "        \"\"\"\n",
    "        modules = []\n",
    "        modules.append(nn.ReLU(inplace=True))\n",
    "        modules.append(SeparableConv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=1))\n",
    "        modules.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        return modules\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass de un bloque.\n",
    "        \"\"\"\n",
    "        # No cambiar\n",
    "        Fx = self.modules(x)\n",
    "\n",
    "        if not self.skip_forward:\n",
    "            Fx += x\n",
    "        else:\n",
    "            Fx += self.skip_forward(x)\n",
    "\n",
    "        return Fx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 6\n",
    "Por último, complete la clase Xception que instancia el modelo completo. Para esto, tendrá que usar la clase XceptionBlock, instanciando cada etapa por separado en la inicialización de la clase y rellenando las funciones para cada flujo. Note que el método Xception.forward ya está implementado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xception(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "\n",
    "        super(Xception, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Head de clasificacion para ilustrar\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        # Aqui debe agregar todos los bloques y etapas\n",
    "        # que necesitara para las funciones de cada flujo.\n",
    "        # ...\n",
    "\n",
    "    def entry_flow(self, x):\n",
    "        \"\"\"\n",
    "        Flujo de entrada (debera usar la funcion\n",
    "        XceptionBlock tres veces mas las dos capas\n",
    "        convolucionales vistas en el enunciado)\n",
    "        \"\"\"\n",
    "        # Agregar flujo de entrada\n",
    "        # ...\n",
    "        # Primeros dos bloques\n",
    "        first_block = [\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        second_block = [\n",
    "            XceptionBlock(64, 128, start_with_relu=False,\n",
    "                          residual_connection=False, last_layer=None),\n",
    "            XceptionBlock(64, 128, start_with_relu=True,\n",
    "                          residual_connection=True, last_layer='MaxPooling'),\n",
    "        ]\n",
    "        third_block = [\n",
    "            XceptionBlock(128, 256, start_with_relu=True,\n",
    "                          residual_connection=False, last_layer=False),\n",
    "            XceptionBlock(128, 256, start_with_relu=True,\n",
    "                          residual_connection=True, last_layer='MaxPooling')\n",
    "        ]\n",
    "        fourth_block = [\n",
    "            XceptionBlock(256, 728, start_with_relu=True,\n",
    "                          residual_connection=False, last_layer=None),\n",
    "            XceptionBlock(256, 728, start_with_relu=True,\n",
    "                          residual_connection=True, last_layer='MaxPooling')\n",
    "        ]\n",
    "\n",
    "        # Agregue los módulos según corresponda\n",
    "        # ...\n",
    "        modules = nn.Sequential(\n",
    "            *first_block, *second_block, *third_block, *fourth_block).to(get_default_device())\n",
    "\n",
    "        return modules\n",
    "\n",
    "    def middle_flow(self, x):\n",
    "        \"\"\"\n",
    "        Flujo intermedio (recordar que se repite 8 veces\n",
    "        un XceptionBlock de determinados parametros)\n",
    "        \"\"\"\n",
    "        # Agregar flujo intermedio\n",
    "        # ...\n",
    "        block = [\n",
    "            XceptionBlock(728, 728, start_with_relu=True,\n",
    "                          residual_connection=False, last_layer=None),\n",
    "            XceptionBlock(728, 728, start_with_relu=True,\n",
    "                          residual_connection=False, last_layer=None),\n",
    "            XceptionBlock(728, 728, start_with_relu=True,\n",
    "                          residual_connection=True, last_layer=None),\n",
    "        ]\n",
    "\n",
    "        full_block = []\n",
    "        full_block.extend([block] * 8)\n",
    "\n",
    "        modules = nn.Sequential(*full_block).to(get_default_device())\n",
    "\n",
    "        return modules\n",
    "\n",
    "    def exit_flow(self, x):\n",
    "        \"\"\"\n",
    "        Flujo de salida (tener en cuenta cambio de\n",
    "        dimensiones entre sub-bloques y funcion de\n",
    "        salida)\n",
    "        \"\"\"\n",
    "        # Agregar flujo de salida\n",
    "        # ...\n",
    "        first_block = [\n",
    "            XceptionBlock(728, 728, start_with_relu=True,\n",
    "                          residual_connection=False, last_layer=None),\n",
    "            XceptionBlock(728, 1024, start_with_relu=True,\n",
    "                          residual_connection=True, last_layer='MaxPooling')\n",
    "        ]\n",
    "        second_block = [\n",
    "            XceptionBlock(1024, 1536, start_with_relu=False,\n",
    "                          residual_connection=False, last_layer=None),\n",
    "            nn.ReLU(inplace=True),\n",
    "            XceptionBlock(1536, 2048, start_with_relu=False,\n",
    "                          residual_connection=False, last_layer=None),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        modules = nn.Sequential(\n",
    "            *first_block, *second_block).to(get_default_device())\n",
    "\n",
    "        # No olvidar etapa de Global Avg. Pooling\n",
    "        # Hint: https://discuss.pytorch.org/t/global-average-pooling-in-pytorch/6721/8\n",
    "        # x = GlobalAvgPooling(x)\n",
    "        modules.append(F.adaptive_avg_pool2d(x, (1,1)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.entry_flow(x)\n",
    "        x = self.middle_flow(x)\n",
    "        x = self.exit_flow(x)\n",
    "        # Hay que agregar un cambio de dimensiones para poder\n",
    "        # pasar x por la capa Fully Connected\n",
    "        # x = ...\n",
    "        x = nn.Flatten(-1, 2048)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 7\n",
    "Usando la función torchsummary.summary(), visualice y verifique que los tamaños de los outputs de\n",
    "cada flujo sean los correctos. Explicite el nombre exacto de cada capa y sus dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# inputs, labels = inputs.to(device), labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_xception \u001b[39m=\u001b[39m Xception(\u001b[39m2048\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m summary(model_xception\u001b[39m.\u001b[39;49mto(get_default_device()), input_size\u001b[39m=\u001b[39;49m(\u001b[39m3\u001b[39;49m,\u001b[39m300\u001b[39;49m,\u001b[39m300\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[1;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb Cell 21\u001b[0m in \u001b[0;36mXception.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_flow(x)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmiddle_flow(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexit_flow(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     \u001b[39m# Hay que agregar un cambio de dimensiones para poder\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     \u001b[39m# pasar x por la capa Fully Connected\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     \u001b[39m# x = ...\u001b[39;00m\n",
      "\u001b[1;32m/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb Cell 21\u001b[0m in \u001b[0;36mXception.middle_flow\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m full_block \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m full_block\u001b[39m.\u001b[39mextend([block] \u001b[39m*\u001b[39m \u001b[39m8\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m modules \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mSequential(\u001b[39m*\u001b[39;49mfull_block)\u001b[39m.\u001b[39mto(get_default_device())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexander/Documents/Doctorado/AprendizajeProfundo/Tareas/tarea1.ipynb#X24sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:91\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[39mfor\u001b[39;00m idx, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(args):\n\u001b[0;32m---> 91\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_module(\u001b[39mstr\u001b[39;49m(idx), module)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:442\u001b[0m, in \u001b[0;36mModule.add_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Adds a child module to the current module.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \n\u001b[1;32m    434\u001b[0m \u001b[39mThe module can be accessed as an attribute using the given name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m    module (Module): child module to be added to the module.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(module, Module) \u001b[39mand\u001b[39;00m module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not a Module subclass\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    443\u001b[0m         torch\u001b[39m.\u001b[39mtypename(module)))\n\u001b[1;32m    444\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(name, torch\u001b[39m.\u001b[39m_six\u001b[39m.\u001b[39mstring_classes):\n\u001b[1;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule name should be a string. Got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    446\u001b[0m         torch\u001b[39m.\u001b[39mtypename(name)))\n",
      "\u001b[0;31mTypeError\u001b[0m: list is not a Module subclass"
     ]
    }
   ],
   "source": [
    "model_xception = Xception(2048)\n",
    "summary(model_xception.to(get_default_device()), input_size=(3,300,300))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación y entrenamiento de un clasificador de perros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# model.cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
