{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/.local/lib/python3.10/site-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /home/alexander/.cache/torch/hub/master.zip\n",
      "YOLOv5 🚀 2023-4-22 Python-3.10.11 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce GTX 1050 Ti with Max-Q Design, 4041MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /home/alexander/.cache/torch/hub/requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████| 14.1M/14.1M [00:00<00:00, 33.8MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>confidence</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>742.974854</td>\n",
       "      <td>48.395508</td>\n",
       "      <td>1141.844482</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>0.881052</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>442.007629</td>\n",
       "      <td>437.522400</td>\n",
       "      <td>496.653992</td>\n",
       "      <td>709.973572</td>\n",
       "      <td>0.675213</td>\n",
       "      <td>27</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123.024139</td>\n",
       "      <td>193.287384</td>\n",
       "      <td>715.662231</td>\n",
       "      <td>719.723877</td>\n",
       "      <td>0.665815</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>982.803162</td>\n",
       "      <td>308.417358</td>\n",
       "      <td>1027.365845</td>\n",
       "      <td>419.987000</td>\n",
       "      <td>0.260075</td>\n",
       "      <td>27</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         xmin        ymin         xmax        ymax  confidence  class    name\n",
       "0  742.974854   48.395508  1141.844482  720.000000    0.881052      0  person\n",
       "1  442.007629  437.522400   496.653992  709.973572    0.675213     27     tie\n",
       "2  123.024139  193.287384   715.662231  719.723877    0.665815      0  person\n",
       "3  982.803162  308.417358  1027.365845  419.987000    0.260075     27     tie"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# Image\n",
    "im = 'https://ultralytics.com/images/zidane.jpg'\n",
    "\n",
    "# Inference\n",
    "results = model(im)\n",
    "\n",
    "results.pandas().xyxy[0]\n",
    "#      xmin    ymin    xmax   ymax  confidence  class    name\n",
    "# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n",
    "# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n",
    "# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n",
    "# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/alexander/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2023-4-22 Python-3.10.11 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce GTX 1050 Ti with Max-Q Design, 4041MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /home/alexander/.cache/torch/hub/requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165k/165k [00:00<00:00, 8.08MB/s]\n",
      "100%|██████████| 476k/476k [00:00<00:00, 12.1MB/s]\n",
      "image 1/2: 720x1280 2 persons, 2 ties\n",
      "image 2/2: 1080x810 4 persons, 1 bus\n",
      "Speed: 13.7ms pre-process, 23.2ms inference, 1.1ms NMS per image at shape (2, 3, 640, 640)\n",
      "Saved 2 images to \u001b[1mruns/detect/exp\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>confidence</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>742.550171</td>\n",
       "      <td>48.037018</td>\n",
       "      <td>1141.204712</td>\n",
       "      <td>716.641724</td>\n",
       "      <td>0.881825</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>442.059509</td>\n",
       "      <td>437.528076</td>\n",
       "      <td>496.809326</td>\n",
       "      <td>709.838989</td>\n",
       "      <td>0.687342</td>\n",
       "      <td>27</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125.191406</td>\n",
       "      <td>193.680664</td>\n",
       "      <td>711.992615</td>\n",
       "      <td>713.046997</td>\n",
       "      <td>0.639421</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>982.893127</td>\n",
       "      <td>308.356750</td>\n",
       "      <td>1027.368774</td>\n",
       "      <td>420.091736</td>\n",
       "      <td>0.262013</td>\n",
       "      <td>27</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         xmin        ymin         xmax        ymax  confidence  class    name\n",
       "0  742.550171   48.037018  1141.204712  716.641724    0.881825      0  person\n",
       "1  442.059509  437.528076   496.809326  709.838989    0.687342     27     tie\n",
       "2  125.191406  193.680664   711.992615  713.046997    0.639421      0  person\n",
       "3  982.893127  308.356750  1027.368774  420.091736    0.262013     27     tie"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# Images\n",
    "for f in 'zidane.jpg', 'bus.jpg':\n",
    "    torch.hub.download_url_to_file('https://ultralytics.com/images/' + f, f)  # download 2 images\n",
    "im1 = Image.open('zidane.jpg')  # PIL image\n",
    "im2 = cv2.imread('bus.jpg')[..., ::-1]  # OpenCV image (BGR to RGB)\n",
    "\n",
    "# Inference\n",
    "results = model([im1, im2], size=640) # batch of images\n",
    "\n",
    "# Results\n",
    "results.print()  \n",
    "results.save()  # or .show()\n",
    "\n",
    "results.xyxy[0]  # im1 predictions (tensor)\n",
    "results.pandas().xyxy[0]  # im1 predictions (pandas)\n",
    "#      xmin    ymin    xmax   ymax  confidence  class    name\n",
    "# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n",
    "# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n",
    "# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n",
    "# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 14\n",
    "Investigue sobre la arquitectura de los modelos YOLO, y explique, en no más de tres lı́neas cada uno, los siguientes conceptos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diferencia entre frameworks de detección de objetos de una y dos etapas (single-stage vs. two-stage object detection).**\n",
    "\n",
    "Los modelos two-stage object detection tienen dos etapas:\n",
    "* Propuesta de región, realizado por lo que se denomina Region Proposal Network (RPN), la cual \"decide\" donde mirar para reducir los requerimientos computacionales de todo el proceso de inferencia. La RPN escanea toda locación para evaluar si se requiere procesar una determinada región. Lo hace proponiedo k boxes cada una con dos puntajes (scores) representando la probabilidad o no de un objeto. \n",
    "* Clasificación del contenido de los boxes y ajuste de sus coordenadas.\n",
    "Ejemplo: Faster RCNN. \n",
    "\n",
    "Los modelos single-stage detection no tienen una etapa de propuesta de región y entregan una localización final y predicción de contenido a la vez. Esto hace que estos modelos sean más rápidos pero menos precisos. Ejemplo: YOLOv5.\n",
    "\n",
    "**Función de pérdida Complete Intersection Over Union (BCE) y su uso en la red YOLOv5.**\n",
    "\n",
    "IoU es una métrica usada para evaluar la performance de un algoritmo en detectar objetos dentro de una imagen. Es calculada por el ratio de la intersección (overlap) de la bounding box predecida y su ground truth, sobre la unión de ambos boxes. Donde 1 indica un overlap perfecto y 0 que no hay overlap. $IoU(A,B)=\\frac{A \\cap B}{A \\cup B}$. Yolo utiliza esta métrica para el cálculo de la precisión promedio utilizando un umbral de precisión en un rango de 0.5 a 0.95.\n",
    "\n",
    "**Función de pérdida Binary Cross Entropy (BCE) y su uso en la red YOLOv5.**\n",
    "\n",
    "\n",
    "\n",
    "**¿Cómo se calcula la función de pérdida total en YOLOv5?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 15\n",
    "Investigue sobre el output de la red YOLOv5 y cómo se traduce el tensor de salida a bounding boxes y detec-\n",
    "ciones de objetos. Además, explique cómo –en general– se obtiene solamente una detección por objeto, y no\n",
    "varias para todas las regiones donde el objeto está presente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 16\n",
    "Instancie el modelo y obtenga la salida para tres fotos a elegir por usted (existen fotos pre-cargadas en Py-\n",
    "Torch si quieren simplicidad). Para esto, complete y utilice las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "def run_inference(model, img, thresh):\n",
    "\"\"\"\n",
    "Dado un modelo, una imagen y un umbral, retorna todos los Bounding Boxes y\n",
    "clases detectada en la imagen que estan por sobre el umbral.\n",
    "\"\"\"\n",
    "# Deben cambiar la transformada para que funcione\n",
    "# transform = Compose[...]\n",
    "# img = transform(img)\n",
    "# Obtener predicciones\n",
    "model = model.eval()\n",
    "predictions = model(img)\n",
    "# Desempaquetar predicciones y filtrar usando el umbral\n",
    "detections = {}\n",
    "# ...\n",
    "return detections\n",
    "def visualize_detections(detections):\n",
    "\"\"\"\n",
    "Dado un diccionario de detecciones, plotea la imagen y cada Bounding Box,\n",
    "junto con la clase y Objectness Score de cada deteccion\n",
    "\"\"\"\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
